{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark로 Spark 다뤄보기\n",
    "\n",
    "\n",
    "이전 게시글\n",
    "\n",
    "- [빅데이터 연대기](https://butter-shower.tistory.com/163)\n",
    "- [빅데이터 양대산맥, Hadoop Ecosystem과 Spark Ecosystem](https://butter-shower.tistory.com/164)\n",
    "- [Spark의 데이터 처리 : RDD(Resilient Distributed Dataset)](https://butter-shower.tistory.com/165?category=737935)\n",
    "\n",
    "\n",
    "이번에는 로컬에서 한번 Spark를 다뤄봅시다. 원래 Spark는 AWS, GCP, Azure 등의 클라우드 환경에서 돌리는 것이 일반적이지만, 스파크에서 제공하는 파이선 API인 PySpark를 활용해서 간단한 실습을 로컬에서 진행할 수 있습니다. \n",
    "\n",
    "그럼 PySpark를 설치해줍시다. \n",
    "\n",
    "## 1. PySpark 설치하기\n",
    "\n",
    "PySpark를 이용하기 위해서는 아래와 같은 패키지가 설치되어 있어야 합니다.\n",
    "\n",
    "- java(>=8.0)\n",
    "- Spark(>=2.2.0)\n",
    "- Python (>=3.4.0)\n",
    "\n",
    "#### 자바 버전 확인\n",
    "\n",
    "자바 버전을 확인해줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.8\" 2020-07-14\r\n",
      "OpenJDK Runtime Environment (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1)\r\n",
      "OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1, mixed mode, sharing)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 설치되어 있네요.\n",
    "\n",
    "#### Spark 설치\n",
    "\n",
    "터미널을 열어 아래 명령어를 통해 Spark를 다운로드 해줍시다.\n",
    "\n",
    "```bash\n",
    "$ wget http://mirror.apache-kr.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "$ tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "#### Spark 실행\n",
    "\n",
    "스파크가 설치된 폴더로 이동하고 bin폴더로도 이동합니다.\n",
    "\n",
    "그리고 spark 실행 명령어 `./spark-shell`를 입력해줍시다.\n",
    "\n",
    "```bash\n",
    "$ cd spark-3.0.1-bin-hadoop2.7\n",
    "$ cd bin\n",
    "$ ./spark-shell\n",
    "```\n",
    "\n",
    "정상 설치가 되었다면 pyspark를 설치해줍시다.\n",
    "\n",
    "```bash\n",
    "$ pip install pyspark\n",
    "```\n",
    "\n",
    "설치 후 버전을 확인해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext를 통한 스파크 초기화\n",
    "\n",
    "### 스파크의 엔트리포인트 SparkContext 객체 선언\n",
    "\n",
    "분산 환경에서 운영되는 스파크는 driver 프로그램을 구동시킬 때 SparkContext라는 특수 객체를 만들게 됩니다. 스파크는 이 SparkContext 객체를 통해 스파크의 모든 기능에 접근합니다. 이 객체는 스파크 프로그램당 한번만 실행할 수 있고 사용 후에는 종료해야 합니다. 따라서 SparkContext는 다른 말로 스파크의 \"엔트리포인트(entry point)\"라고도 하고, SparkContext를 생성하는 것을 \"스파크를 초기화한다(Initializing Spark)\"라고 합니다.\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-17.max-800x600.png)\n",
    "\n",
    "### PySpark에서는..\n",
    "\n",
    "PySpark에서 선언하는 SparkContext 객체는 내부의 JVM(Java Virtual Machine)위에 동작하는 Py4J의 SparkContext와 연결됩니다. 이 Py4J의 SparkContext는 Worker 노드들과도 연결되어 있고 이 Worker 녿들 역시 실제 동작은 JVM 위애서 동작합니다. Worker 노드들의 조작 역시 Python을 통해 할 수 있습니다만 실제 동작은 다 JVM위에서 행해집니다. 한마디로, PySpark는 Python으로 코딩을 하지만 실제 동작은 JVM에 의해서 행해지고 있습니다.\n",
    "\n",
    "**공식 문서 확인**\n",
    "\n",
    "아래는 PySpark에서 설명하는 SparkContext 객체 사양입니다.\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-21.max-800x600.png)\n",
    "\n",
    "위 문서를 통해 확인할 수 있는 SparkContext의 주요 내용은 아래와 같습니다.\n",
    "\n",
    "- 문법 : `pyspark.SparkContext()`\n",
    "- 스파크 기능의 기본 엔트리 포인트입니다.\n",
    "- 스파크 클러스터와 연결을 나타내며 RDD를 만들고 브로드캐스트하는데 사용될 수 있습니다.\n",
    "- JVM당 하나만 활성화해야하며, 새로운 것을 만들기 전에는 활성을 중지해야 합니다.\n",
    "\n",
    "그럼 시작해봅시다!\n",
    "\n",
    "\n",
    "## Hands-on!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모듈 import\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 변수명은 `sc`로 선언합니다. \n",
    "\n",
    "SparkContext 객체 타입을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 만들어졌습니다.\n",
    "\n",
    "하지만 실수로 SparkContext를 한개 더 만들면 에러가 납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-4d9b047b29c1>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fb091d993a42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#에러가 발생합니다!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-4d9b047b29c1>:4 "
     ]
    }
   ],
   "source": [
    "#에러가 발생합니다!\n",
    "new_sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code-2\n",
    "\n",
    "SparkContext의 Configuration을 세팅할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext(master = 'local', appName='PySpark Basic')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Web-UI 버전을 이용하여 로컬 모드에서 사용할 경우 master url은 http://localhost:8080 으로 설정합니다. 우리는 쥬피터 노트북에서 실행할 것이므로 위와 같이 master=local로 설정하면 됩니다. 상세 내용은 아래 공식홈페이지의 설명을 참고 하세요.\n",
    "\n",
    "<https://spark.apache.org/docs/latest/spark-standalone.html>\n",
    "\n",
    "생성한 SparkContext의 Configuration을 확인하기 위해서 `.getConf().getAll()`을 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.name', 'PySpark Basic'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '192.168.0.8'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1600673692546'),\n",
       " ('spark.driver.port', '37379'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나씩도 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark Basic'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code - 3\n",
    "\n",
    "`SparkConf()`를 이용해 SparkContext의 Configuration을 설정하는 방법을 이용해서 SparkContext를 만들 수 있습니다.\n",
    "\n",
    "`.setMaster()`, `.setAppName()`을 이용해 어플리케이션의 이름과 Master의 URL을 설정해줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('PySpark Basic').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세가지 모두 많이 사용하는 코딩 스타일입니다! 잘 알아두시길 바랍니다. :)\n",
    "\n",
    "## RDD Creation\n",
    "\n",
    "### (1) 내부에서 만들어진 데이터 집합을 병렬화 하는 방법 : `parallelize()` 함수 사용\n",
    "\n",
    "방금 전 만든 SparkContext()의 parallelize() 함수를 사용하여 내부의 데이터 집합을 RDD로 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그냥 설명만 출력됩니다.\n",
    "\n",
    "데이터 타입을 한번 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 생성은 된 것 같습니다.\n",
    "\n",
    "![Img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-15.max-800x600.png)\n",
    "\n",
    "위 그림을 다시 한번 봅시다.\n",
    "\n",
    "RDD는 생성과 transformation 연산을 바로 수행하지 않습니다. 이 단계에서는 연산을 하고있지 않고 계보(lineage)만 만들어놓고 actions 동작을 할 때 rdd가 비로소 만들어지며, 이를 *느긋한 계산법* 이라고 했습니다.\n",
    "\n",
    "즉, 지금은 RDD를 생성했지만 RDD가 만들어지지는 않았습니다.\n",
    "\n",
    "actions를 해봅시다.\n",
    "\n",
    "함수는 `take()`라는 함수를 사용하겠습니다. RDD의 원소를 반환합니다. 인자로는 반환하고자하는 RDD의 원소의 갯수를 입력합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 외부의 파일을 로드하는 방법 : `.textFile()` 함수 사용\n",
    "\n",
    "`.textFile()` 함수를 이용해 외부 파일을 로드하며 rdd를 만들 수 있습니다.\n",
    "\n",
    "우선 간단히 파일 하나 만들어보겠습니다. 우리는 `./data`  파일에 만들어보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = os.path.dirname(os.path.abspath('__file__')) + r'/data/test.txt'\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    for i in range(10):\n",
    "        f.write(str(i) + '\\n')\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 잘 만들어졌나요? 그럼 방금 만든 파일을 불러와 RDD를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel0039/Documents/GitHub/aiffel/Week8/data/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.textFile(file_path)\n",
    "print(rdd2)\n",
    "print(type(rdd2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actions를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한가지 특이사항이 있습니다.\n",
    "\n",
    "우리는 숫자를 입력했으므로 위의 결과가 [0, 1, 2]가 될 줄 알았는데 문자열의 list가 얻어졌습니다.\n",
    "\n",
    "이것은 spark가 `.textFile()`을 통해 얻은 데이터 타입을 무조건 string으로 처리하기 때문에 그렇습니다. 그렇다면 이 데이터를 숫자로 변환하려면 어떻게 해야할까요? \n",
    "\n",
    "바로 다음 스텝에 나오는 Transformation 같은 RDD Operation이 필요한 것입니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## RDD Operation - (1) Transformations\n",
    "\n",
    "이어서 RDD 동작에 대해 연습해보도록 하겠습니다.\n",
    "\n",
    "RDD의 연산은 NumPy 연습하듯이 자주 사용되는 함수들을 간단한 예제코드를 통해 학습하는 것이 좋습니다. 우선 기본이 되는 몇가지 동작을 설명하여 RDD 연산 과정을 확인해보고 동영상 자료를 통해 여러가지 예제를 익혀봅시다.\n",
    "\n",
    "### Transformations\n",
    "\n",
    "- map()\n",
    "- filter()\n",
    "- flatmap()\n",
    "\n",
    "#### map\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-23.max-800x600.png)\n",
    "\n",
    "x의 모든 원소에 대해 map 함수를 적용한 결과는 y값입니다. 따라서 x와 y의 원소 갯수는 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c', 'd']\n",
      "[('b', 1), ('a', 1), ('c', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(['b', 'a', 'c', 'd'])\n",
    "y = x.map(lambda z : (z, 1))\n",
    "print(x.collect()) # collect()는 actions입니다.\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(문자, 1)` 형식으로 map 했습니다!! 아래 예제도 한번 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3])\n",
    "squares = nums.map(lambda x : x*x)\n",
    "print(squares.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-24.max-800x600.png)\n",
    "\n",
    "filter 연산은 어떤 조건을 만족하는 값만을 반환합니다. 따라서 조건문이 들어가야하며, x와 y의 원소의 갯수는 같지 않을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3, 4, 5])\n",
    "y = x.filter(lambda x : x%2 == 0)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2의 배수만 출력하는 필터였습니다. 아래 예제도 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "text = sc.parallelize(['a', 'b', 'c', 'd'])\n",
    "capital = text.map(lambda x : x.upper())\n",
    "A = capital.filter(lambda x : 'A' in x)\n",
    "print(text.collect())\n",
    "print(A.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatmap\n",
    "\n",
    "아래는 flatmap의 연산 과정을 나타낸 그림입니다.\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-25.max-800x600.png)\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-26.max-800x600.png)\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-27.max-800x600.png)\n",
    "\n",
    "![img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-28.max-800x600.png)\n",
    "\n",
    "FlatMap은 RDD의 원소에 map 연산을 수행하고 원소의 갯수를 증가시키기도 합니다. 원소의 갯수는 꼭 동일하게 증가시키지 않아도 됩니다.\n",
    "\n",
    "![Img](https://aiffelstaticprd.blob.core.windows.net/media/images/F-39-29.max-800x600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 10, 30, 2, 20, 30, 3, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3])\n",
    "y = x.flatMap(lambda x : (x, x*10, 30))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 예제는 RDD Transformation 몇가지를 1개 라인에 중첩 적용한 경우입니다. 언뜻 복집해 보이겠지만 적용된 Transformation 함수의 효과를 하나씩 따로 적용해서 collect()로 결과를 확인해보면 어렵지 않게 파악 가능할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'is',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'by',\n",
       " 'python']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsDataset = sc.parallelize(['Spark is funny', 'It is beautiful', 'And also It is implemented by python'])\n",
    "result = wordsDataset.flatMap(lambda x : x.split()).filter(lambda x : x != ' ').map(lambda x : x.lower())\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV 파일 읽어들이기\n",
    "\n",
    "위에서 살펴본 RDD transformation 함수들을 토대로 좀더 실전적인 예제를 다루어 봅시다. 우리가 다루는 많은 데이터들은 주로 csv 파일로 되어 있습니다. 유명한 Titanic 데이터셋 파일을 스파크로 읽어들이는 것을 연습해 봅시다. 우선 작업환경에 아래와 같이 csv 파일을 다운받아 둡시다.\n",
    "\n",
    "```bash\n",
    "$ wget https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
    "$ mv train.csv ./data\n",
    "```\n",
    "파일을 불러들여 RDD를 생성하는 방법은 이미 알고 있습니다. sc.textFile()를 아래와 같이 활용해 봅시다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone',\n",
       " '0,male,22.0,1,0,7.25,Third,unknown,Southampton,n',\n",
       " '1,female,38.0,1,0,71.2833,First,C,Cherbourg,n',\n",
       " '1,female,26.0,0,0,7.925,Third,unknown,Southampton,y',\n",
       " '1,female,35.0,1,0,53.1,First,C,Southampton,n']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "csv_path = os.path.dirname(os.path.abspath('__file__')) + r'/data/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_0.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일을 그대로 읽어서 상위 5라인만 출력해 보았습니다. 이것을 데이터셋으로 만들려면, 1번째 라인의 컬럼 부분을 분리해 내고, 매 데이터 라인마다 [(column1, 데이터1), (column2, 데이터2)…] 의 list 형태로 바꿔 주고 싶습니다. 좀더 데이터를 가공해 봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비어있는 라인은 제외하고, delimeter인 ,로 line을 분리해 줍니다. \n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "csv_data_1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼 부분만 아래와 같이 분리해 낼 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = csv_data_1.take(1)\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼을 제외한 나머지 데이터만 분리해 낼 방법이 필요합니다. 데이터의 첫번째 컬럼은 0 또는 1의 숫자로만 이루어져 있습니다. 이 조건을 filter로 활용하면 컬럼 부분을 제외할 수 있을 것 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '28.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8.4583',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Queenstown',\n",
       "  'y']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_2 = csv_data_1.filter(lambda line : line[0].isdecimal()) # 첫번째 칼럼이 숫자인지\n",
    "csv_data_2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거의 다 왔습니다. 이제 칼럼 기준으로 `csv_data_2`를 정리해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '35.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '53.1'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '28.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '8.4583'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Queenstown'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_3 = csv_data_2.map(lambda line : [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "csv_data_3.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 원하는 형태로 csv 파일이 가공되었습니다. 이 형태라면 다음 스텝에 나올 다양한 Action 함수를 적용하여 다양하게 분석해볼 수 있습니다.\n",
    "\n",
    "하지만 csv 파일을 꼭 이렇게 가공해야 할까요? 꼭 그렇지는 않습니다. 마지막으로 csv 파일을 DataFrame으로 읽어들이는 방법을 소개해드리겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class|deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|0       |male  |22.0|1                 |0    |7.25   |Third|unknown|Southampton|n    |\n",
      "|1       |female|38.0|1                 |0    |71.2833|First|C      |Cherbourg  |n    |\n",
      "|1       |female|26.0|0                 |0    |7.925  |Third|unknown|Southampton|y    |\n",
      "|1       |female|35.0|1                 |0    |53.1   |First|C      |Southampton|n    |\n",
      "|0       |male  |28.0|0                 |0    |8.4583 |Third|unknown|Queenstown |y    |\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext, SparkFiles\n",
    "\n",
    "url = 'https://storage.googleapis.com/tf-datasets/titanic/train.csv'\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.csv(SparkFiles.get('train.csv'), header = True, inferSchema = True)\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 우리가 사용했던 SparkContext를 한번 더 가공한 SQLContext에서 제공하는 `read.csv()` 함수를 이용하면 스파크의 DataFrame을 얻을 수 있습니다. 이것은 우리에게 아주 익숙한 Pandas의 Dataframe과 똑같지는 않지만 매우 유사합니다. 실제로 SQLContext에는 RDD를 이용해 데이터를 분석하는 것 보다 훨씬 편리하고 강력한 기능들을 많이 제공하고 있습니다.\n",
    "\n",
    "관련해서 자세한 내용은 아래 링크를 참고하세요!\n",
    "\n",
    "<https://spark.apache.org/docs/1.6.1/sql-programming-guide.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class |deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|0       |male  |66.0|0                 |0    |10.5   |Second|unknown|Southampton|y    |\n",
      "|0       |male  |42.0|1                 |0    |52.0   |First |unknown|Southampton|n    |\n",
      "|1       |female|49.0|1                 |0    |76.7292|First |D      |Cherbourg  |n    |\n",
      "|0       |male  |65.0|0                 |1    |61.9792|First |B      |Cherbourg  |n    |\n",
      "|0       |male  |45.0|1                 |0    |83.475 |First |C      |Southampton|n    |\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위에서 얻은 데이터에서 40세 이상인 사람들의 데이터만 필터링해 봅시다. \n",
    "df2 = df[df['age'] > 40]\n",
    "df2.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD - Transformations 더 많은 함수들\n",
    "\n",
    "더 많은 Transformations 함수들이 있습니다. 아래 링크에서 확인하세요!\n",
    "\n",
    "<https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operation - (2) Actions\n",
    "\n",
    "**Actions**\n",
    "\n",
    "- collect()\n",
    "- take()\n",
    "- count()\n",
    "- reduce()\n",
    "- saveAsTextFile()\n",
    "\n",
    "`collect()`, `take()`는 이미 몇번 사용해봤던 익숙한 Action들입니다.\n",
    "\n",
    "#### collect\n",
    "\n",
    "RDD 내의 모든 값을 리턴합니다. 만약 정말 빅데이터를 다루고 있다면 함부로 호출하지 않는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize(list(range(10)))\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take\n",
    "\n",
    "RDD에서 앞쪽 n개의 데이터 list를 리턴합니다. collect()보다 안전하게 데이터를 확인해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count\n",
    "\n",
    "RDD에 포함된 데이터 갯수를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce\n",
    "\n",
    "드디어 reduce 함수가 나왔습니다. MapReduce의 그 reduce에 해당합니다.\n",
    "\n",
    "Map은 Transformations 함수로, Reduce는 Actions 함수로 구현했습니다. Reduce 할 데이터가 RDD에 메모리 상에 존재하므로 이전의 다른 구현체보다 훨씬 빠르게 MapReduce를 실행할 수 있겠습니다.\n",
    "\n",
    "아래는 RDD의 모든 데이터를 차례차례 더하는 sum()을 구현한 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.reduce(lambda x, y : x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saveAsTextFile\n",
    "\n",
    "RDD 데이터를 파일로 저장합니다. 아래 코드를 실행하면 `file.txt`라는 파일에 RDD 내용이 저장될 것입니다. 과연 그럴까요?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.dirname(os.path.abspath('__file__')) + r'/data/file.txt'\n",
    "nums.saveAsTextFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5256\r\n",
      "-rw-rw-r-- 1 aiffel0039 aiffel0039 1152695  9월 17 14:22 cat.jpg\r\n",
      "-rw-rw-r-- 1 aiffel0039 aiffel0039 1944447  9월 17 14:22 dog.jpg\r\n",
      "drwxrwxr-x 2 aiffel0039 aiffel0039    4096  9월 21 18:05 file.txt\r\n",
      "-rw-rw-r-- 1 aiffel0039 aiffel0039  683114  9월 10 14:57 img.JPG\r\n",
      "-rw-rw-r-- 1 aiffel0039 aiffel0039  730433  9월 17 14:22 my_pic1.JPG\r\n",
      "-rw-rw-r-- 1 aiffel0039 aiffel0039  822069  9월 17 14:22 my_pic2.JPG\r\n",
      "-rw-rw-r-- 1 aiffel0039 aiffel0039      20  9월 21 17:15 test.txt\r\n",
      "-rw-r--r-- 1 aiffel0039 aiffel0039   30874  2월 21  2019 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file.txt` 내용을 잘 보시면 놀랍게도 디렉토리 타입으로 생성되어 있습니다. 이 디렉토리 안에 들어가보면 `part-00000`라는 이름의 텍스트 파일이 생성되어 있어서, 실제 우리가 기록하고 싶었던 내용은 이 파일 안에 있습니다. \n",
    "\n",
    "왜 이런 일이 생겼을까요?\n",
    "\n",
    "우리가 다루고있는 스파크가 바로 분산형 빅데이터 시스템이기 때문입니다. 스파크가 다룰 파일 사이즈는 하드디스크 하나에 다 담지 못할만큼 큰 경우일 수도 있습니다. 비록 작은 RDD지만 우리는 이미 `sc.parallelize()`를 통해 분산형 데이터로 생성했음을 잊지 마세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .container{width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style> .container{width:90% !important;}</style>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
