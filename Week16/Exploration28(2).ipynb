{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 : 개구리는 안돼요! (CIFAR-10)\n",
    "\n",
    "이번 프로젝트는 지금까지의 실습과 동일한 방법으로 CIFAR-10 데이터셋에 대해 진행해보겠습니다.\n",
    "\n",
    "만들 모델은 CIFAR-10의 10가지 클래스 중 개구리 라벨을 이상 데이터로 처리하는 모델입니다. 혹시 개구리가 출현할 경우 이를 감지하여 이상감지 경과를 발생시키는 개구리 감지 모델이라고 할 수 있겠습니다.\n",
    "\n",
    "아래 순서를 따라 진행해주세요.\n",
    "\n",
    "- 이상 감지용 데이터셋 구축 (개구리 데이터를 학습 데이터셋에서 제외하며 테스트 데이터셋에 포함)\n",
    "- Skip-GANomaly 모델의 구현\n",
    "- 모델의 학습과 검증\n",
    "- 검증 결과의 시각화 (정상 - 이상 데이터의 anomaly score 분포 시각화, 적절한 threshold에 따른 이상감지율 계산, 감지 성공/실패 사례 시각화 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 모듈 import\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 이상감지 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i])\n",
    "    plt.title(f'index:{i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개구리 라벨은 6번입니다. 이상 데이터로 선정된 6번 라벨 데이터를 제외하도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 6:\n",
    "            #print(old_label)\n",
    "            new_t_labels.append([0]) # 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])\n",
    "    return new_t_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == [0]:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50000건의 데이터를 잘 나눈 것 같습니다. 개구리만 5000장이나 있네요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋을 구성하고 label을 검증해봅시다. \n",
    "\n",
    "훈련 데이터셋에는 라벨이 1인 데이터만 존재하고 테스트 데이터에는 0과 1이 섞여있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]], shape=(8, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Skip-GANomaly 모델의 구현\n",
    "\n",
    "\n",
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel = 3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2,\n",
    "                                                  padding='same', use_bias=False,\n",
    "                                                  kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "Discriminator도 Generator처럼 Conv_block을 활용하며, 최종적으로 sigmoid를 거쳐 0~1 사이의 숫자를 리턴합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                    use_bias = False, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 함수\n",
    "\n",
    "GAN 모델의 핵심은 Loss 함수의 구성방법에 달려 있다고 해도 과언이 아닙니다. Skip-GANomaly는 이전 모델들과 달리 일반적인 GAN의 학습 절차와 같은 형태의 Loss 구성이 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, \n",
    "                   input_data, gen_data, \n",
    "                   latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "        w_context * context_loss + \\\n",
    "        w_encoder * encoder_loss\n",
    "\n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 모델 학습과 검증\n",
    "\n",
    "### Model Train\n",
    "\n",
    "본격적으로 모델을 학습시켜봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "    \n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer generator_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer discriminator_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Steps : 100, \t Total Gen Loss : 16.356691360473633, \t Total Dis Loss : 0.6951030492782593\n",
      "Steps : 200, \t Total Gen Loss : 16.629589080810547, \t Total Dis Loss : 0.6449999809265137\n",
      "Steps : 300, \t Total Gen Loss : 15.86461353302002, \t Total Dis Loss : 0.5917054414749146\n",
      "Steps : 400, \t Total Gen Loss : 19.19941520690918, \t Total Dis Loss : 0.29509440064430237\n",
      "Steps : 500, \t Total Gen Loss : 17.099016189575195, \t Total Dis Loss : 0.2623327672481537\n",
      "Steps : 600, \t Total Gen Loss : 16.36342430114746, \t Total Dis Loss : 0.3408002257347107\n",
      "Steps : 700, \t Total Gen Loss : 16.048376083374023, \t Total Dis Loss : 0.37195253372192383\n",
      "Steps : 800, \t Total Gen Loss : 17.309175491333008, \t Total Dis Loss : 0.1719617247581482\n",
      "Steps : 900, \t Total Gen Loss : 16.97923469543457, \t Total Dis Loss : 0.16030538082122803\n",
      "Steps : 1000, \t Total Gen Loss : 20.33144187927246, \t Total Dis Loss : 0.027784397825598717\n",
      "Steps : 1100, \t Total Gen Loss : 18.306947708129883, \t Total Dis Loss : 0.04161907732486725\n",
      "Steps : 1200, \t Total Gen Loss : 20.48210906982422, \t Total Dis Loss : 0.11648516356945038\n",
      "Steps : 1300, \t Total Gen Loss : 15.821781158447266, \t Total Dis Loss : 0.07110145688056946\n",
      "Steps : 1400, \t Total Gen Loss : 17.950197219848633, \t Total Dis Loss : 0.08691800385713577\n",
      "Steps : 1500, \t Total Gen Loss : 21.32716178894043, \t Total Dis Loss : 0.14897993206977844\n",
      "Steps : 1600, \t Total Gen Loss : 20.718860626220703, \t Total Dis Loss : 0.06855916231870651\n",
      "Steps : 1700, \t Total Gen Loss : 19.77260398864746, \t Total Dis Loss : 0.058197248727083206\n",
      "Steps : 1800, \t Total Gen Loss : 16.91268539428711, \t Total Dis Loss : 0.1879613697528839\n",
      "Steps : 1900, \t Total Gen Loss : 19.47414207458496, \t Total Dis Loss : 0.16694897413253784\n",
      "Steps : 2000, \t Total Gen Loss : 20.4783935546875, \t Total Dis Loss : 0.014593455940485\n",
      "Steps : 2100, \t Total Gen Loss : 20.24393081665039, \t Total Dis Loss : 0.11833131313323975\n",
      "Steps : 2200, \t Total Gen Loss : 19.68853187561035, \t Total Dis Loss : 0.03135431185364723\n",
      "Steps : 2300, \t Total Gen Loss : 21.12094497680664, \t Total Dis Loss : 0.011588049121201038\n",
      "Steps : 2400, \t Total Gen Loss : 20.02750587463379, \t Total Dis Loss : 0.011459682136774063\n",
      "Steps : 2500, \t Total Gen Loss : 20.68317985534668, \t Total Dis Loss : 0.00834900327026844\n",
      "Steps : 2600, \t Total Gen Loss : 22.01302719116211, \t Total Dis Loss : 0.19845429062843323\n",
      "Steps : 2700, \t Total Gen Loss : 18.384244918823242, \t Total Dis Loss : 0.0164018627256155\n",
      "Steps : 2800, \t Total Gen Loss : 20.155168533325195, \t Total Dis Loss : 0.09650775790214539\n",
      "Steps : 2900, \t Total Gen Loss : 20.18827247619629, \t Total Dis Loss : 0.009731338359415531\n",
      "Steps : 3000, \t Total Gen Loss : 22.5264949798584, \t Total Dis Loss : 0.006097977980971336\n",
      "Steps : 3100, \t Total Gen Loss : 19.466190338134766, \t Total Dis Loss : 0.09060083329677582\n",
      "Steps : 3200, \t Total Gen Loss : 24.105539321899414, \t Total Dis Loss : 0.03154595568776131\n",
      "Steps : 3300, \t Total Gen Loss : 20.0511531829834, \t Total Dis Loss : 0.2145995944738388\n",
      "Steps : 3400, \t Total Gen Loss : 21.031009674072266, \t Total Dis Loss : 0.006319369655102491\n",
      "Steps : 3500, \t Total Gen Loss : 21.035741806030273, \t Total Dis Loss : 0.03559686616063118\n",
      "Steps : 3600, \t Total Gen Loss : 21.317838668823242, \t Total Dis Loss : 0.005278985947370529\n",
      "Steps : 3700, \t Total Gen Loss : 22.47818946838379, \t Total Dis Loss : 0.004522780887782574\n",
      "Steps : 3800, \t Total Gen Loss : 23.038299560546875, \t Total Dis Loss : 0.01362110860645771\n",
      "Steps : 3900, \t Total Gen Loss : 21.863304138183594, \t Total Dis Loss : 0.005246401764452457\n",
      "Steps : 4000, \t Total Gen Loss : 23.16767692565918, \t Total Dis Loss : 0.006652428302913904\n",
      "Steps : 4100, \t Total Gen Loss : 23.0129451751709, \t Total Dis Loss : 0.008569384925067425\n",
      "Steps : 4200, \t Total Gen Loss : 20.715295791625977, \t Total Dis Loss : 0.008178062736988068\n",
      "Steps : 4300, \t Total Gen Loss : 22.313337326049805, \t Total Dis Loss : 0.07371394336223602\n",
      "Steps : 4400, \t Total Gen Loss : 21.91274070739746, \t Total Dis Loss : 0.005310523323714733\n",
      "Steps : 4500, \t Total Gen Loss : 19.68568229675293, \t Total Dis Loss : 0.010339783504605293\n",
      "Steps : 4600, \t Total Gen Loss : 22.071138381958008, \t Total Dis Loss : 0.00355493719689548\n",
      "Steps : 4700, \t Total Gen Loss : 23.434986114501953, \t Total Dis Loss : 0.00982084684073925\n",
      "Steps : 4800, \t Total Gen Loss : 19.741043090820312, \t Total Dis Loss : 0.004558799788355827\n",
      "Steps : 4900, \t Total Gen Loss : 22.021892547607422, \t Total Dis Loss : 0.005455661565065384\n",
      "Steps : 5000, \t Total Gen Loss : 20.993732452392578, \t Total Dis Loss : 0.002111402340233326\n",
      "Steps : 5100, \t Total Gen Loss : 18.545116424560547, \t Total Dis Loss : 0.015191765502095222\n",
      "Steps : 5200, \t Total Gen Loss : 22.227725982666016, \t Total Dis Loss : 0.004643484950065613\n",
      "Steps : 5300, \t Total Gen Loss : 23.800289154052734, \t Total Dis Loss : 0.0026977756060659885\n",
      "Steps : 5400, \t Total Gen Loss : 24.08199691772461, \t Total Dis Loss : 0.003628496080636978\n",
      "Steps : 5500, \t Total Gen Loss : 21.04743766784668, \t Total Dis Loss : 0.003790607675909996\n",
      "Steps : 5600, \t Total Gen Loss : 24.142019271850586, \t Total Dis Loss : 0.011172262020409107\n",
      "Time for epoch 1 is 290.35519647598267 sec\n",
      "Steps : 5700, \t Total Gen Loss : 21.59862518310547, \t Total Dis Loss : 0.0034605134278535843\n",
      "Steps : 5800, \t Total Gen Loss : 21.634681701660156, \t Total Dis Loss : 0.0014267002698034048\n",
      "Steps : 5900, \t Total Gen Loss : 23.260780334472656, \t Total Dis Loss : 0.007857243530452251\n",
      "Steps : 6000, \t Total Gen Loss : 20.587120056152344, \t Total Dis Loss : 0.028073791414499283\n",
      "Steps : 6100, \t Total Gen Loss : 21.468568801879883, \t Total Dis Loss : 0.009033691138029099\n",
      "Steps : 6200, \t Total Gen Loss : 19.469810485839844, \t Total Dis Loss : 0.4516032338142395\n",
      "Steps : 6300, \t Total Gen Loss : 24.951595306396484, \t Total Dis Loss : 0.010560369119048119\n",
      "Steps : 6400, \t Total Gen Loss : 21.308753967285156, \t Total Dis Loss : 0.006163426674902439\n",
      "Steps : 6500, \t Total Gen Loss : 22.376005172729492, \t Total Dis Loss : 0.004191129002720118\n",
      "Steps : 6600, \t Total Gen Loss : 22.059423446655273, \t Total Dis Loss : 0.0069519877433776855\n",
      "Steps : 6700, \t Total Gen Loss : 22.368770599365234, \t Total Dis Loss : 0.010609113611280918\n",
      "Steps : 6800, \t Total Gen Loss : 21.45404052734375, \t Total Dis Loss : 0.02545516937971115\n",
      "Steps : 6900, \t Total Gen Loss : 21.659076690673828, \t Total Dis Loss : 0.007318055257201195\n",
      "Steps : 7000, \t Total Gen Loss : 19.59079360961914, \t Total Dis Loss : 0.005182735156267881\n",
      "Steps : 7100, \t Total Gen Loss : 23.89142417907715, \t Total Dis Loss : 0.05658078193664551\n",
      "Steps : 7200, \t Total Gen Loss : 19.124677658081055, \t Total Dis Loss : 0.008057261817157269\n",
      "Steps : 7300, \t Total Gen Loss : 20.79673194885254, \t Total Dis Loss : 0.008643396198749542\n",
      "Steps : 7400, \t Total Gen Loss : 23.309463500976562, \t Total Dis Loss : 0.006904815323650837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7500, \t Total Gen Loss : 20.62744140625, \t Total Dis Loss : 0.012571100145578384\n",
      "Steps : 7600, \t Total Gen Loss : 21.312015533447266, \t Total Dis Loss : 0.006426719948649406\n",
      "Steps : 7700, \t Total Gen Loss : 23.053009033203125, \t Total Dis Loss : 0.006357967853546143\n",
      "Steps : 7800, \t Total Gen Loss : 20.81296730041504, \t Total Dis Loss : 0.0044759297743439674\n",
      "Steps : 7900, \t Total Gen Loss : 21.43758201599121, \t Total Dis Loss : 0.003276082221418619\n",
      "Steps : 8000, \t Total Gen Loss : 21.98386573791504, \t Total Dis Loss : 0.001989067532122135\n",
      "Steps : 8100, \t Total Gen Loss : 20.911640167236328, \t Total Dis Loss : 0.0013820463791489601\n",
      "Steps : 8200, \t Total Gen Loss : 24.292871475219727, \t Total Dis Loss : 0.004574926104396582\n",
      "Steps : 8300, \t Total Gen Loss : 20.010971069335938, \t Total Dis Loss : 0.10062025487422943\n",
      "Steps : 8400, \t Total Gen Loss : 23.793867111206055, \t Total Dis Loss : 0.0035297181457281113\n",
      "Steps : 8500, \t Total Gen Loss : 27.462818145751953, \t Total Dis Loss : 0.0024660672061145306\n",
      "Steps : 8600, \t Total Gen Loss : 22.913097381591797, \t Total Dis Loss : 0.02796102687716484\n",
      "Steps : 8700, \t Total Gen Loss : 22.0108642578125, \t Total Dis Loss : 0.005705381743609905\n",
      "Steps : 8800, \t Total Gen Loss : 19.248411178588867, \t Total Dis Loss : 0.02477952651679516\n",
      "Steps : 8900, \t Total Gen Loss : 21.55733299255371, \t Total Dis Loss : 0.017177851870656013\n",
      "Steps : 9000, \t Total Gen Loss : 27.00279426574707, \t Total Dis Loss : 0.001020123716443777\n",
      "Steps : 9100, \t Total Gen Loss : 24.123476028442383, \t Total Dis Loss : 0.003839688142761588\n",
      "Steps : 9200, \t Total Gen Loss : 21.111469268798828, \t Total Dis Loss : 0.003368886187672615\n",
      "Steps : 9300, \t Total Gen Loss : 23.197534561157227, \t Total Dis Loss : 0.003152639837935567\n",
      "Steps : 9400, \t Total Gen Loss : 23.048389434814453, \t Total Dis Loss : 0.0007517541525885463\n",
      "Steps : 9500, \t Total Gen Loss : 19.04526138305664, \t Total Dis Loss : 0.03796353563666344\n",
      "Steps : 9600, \t Total Gen Loss : 21.302349090576172, \t Total Dis Loss : 0.008862209506332874\n",
      "Steps : 9700, \t Total Gen Loss : 21.316852569580078, \t Total Dis Loss : 0.0030172739643603563\n",
      "Steps : 9800, \t Total Gen Loss : 22.90851402282715, \t Total Dis Loss : 0.007523417007178068\n",
      "Steps : 9900, \t Total Gen Loss : 24.900426864624023, \t Total Dis Loss : 0.0019573739264160395\n",
      "Steps : 10000, \t Total Gen Loss : 26.06221580505371, \t Total Dis Loss : 0.001172880525700748\n",
      "Steps : 10100, \t Total Gen Loss : 21.494861602783203, \t Total Dis Loss : 0.003760785097256303\n",
      "Steps : 10200, \t Total Gen Loss : 22.54579734802246, \t Total Dis Loss : 0.005676146596670151\n",
      "Steps : 10300, \t Total Gen Loss : 24.14520835876465, \t Total Dis Loss : 0.003912236541509628\n",
      "Steps : 10400, \t Total Gen Loss : 24.91054344177246, \t Total Dis Loss : 0.004042600281536579\n",
      "Steps : 10500, \t Total Gen Loss : 23.734712600708008, \t Total Dis Loss : 0.10541541874408722\n",
      "Steps : 10600, \t Total Gen Loss : 23.929452896118164, \t Total Dis Loss : 0.0023623970337212086\n",
      "Steps : 10700, \t Total Gen Loss : 21.85708999633789, \t Total Dis Loss : 0.02857455424964428\n",
      "Steps : 10800, \t Total Gen Loss : 24.075605392456055, \t Total Dis Loss : 0.0034173396416008472\n",
      "Steps : 10900, \t Total Gen Loss : 26.35256576538086, \t Total Dis Loss : 0.0018405690789222717\n",
      "Steps : 11000, \t Total Gen Loss : 23.581361770629883, \t Total Dis Loss : 0.0008165869512595236\n",
      "Steps : 11100, \t Total Gen Loss : 22.075456619262695, \t Total Dis Loss : 0.010502017103135586\n",
      "Steps : 11200, \t Total Gen Loss : 22.513769149780273, \t Total Dis Loss : 0.06756702810525894\n",
      "Time for epoch 2 is 279.21731758117676 sec\n",
      "Steps : 11300, \t Total Gen Loss : 23.90752410888672, \t Total Dis Loss : 0.0018219512421637774\n",
      "Steps : 11400, \t Total Gen Loss : 19.5653018951416, \t Total Dis Loss : 0.01437858585268259\n",
      "Steps : 11500, \t Total Gen Loss : 21.21630859375, \t Total Dis Loss : 0.006140072830021381\n",
      "Steps : 11600, \t Total Gen Loss : 24.600177764892578, \t Total Dis Loss : 0.0006261342787183821\n",
      "Steps : 11700, \t Total Gen Loss : 24.31113624572754, \t Total Dis Loss : 0.004609173629432917\n",
      "Steps : 11800, \t Total Gen Loss : 27.288942337036133, \t Total Dis Loss : 0.0026560877449810505\n",
      "Steps : 11900, \t Total Gen Loss : 25.211515426635742, \t Total Dis Loss : 0.0013140075607225299\n",
      "Steps : 12000, \t Total Gen Loss : 20.597797393798828, \t Total Dis Loss : 0.00409771129488945\n",
      "Steps : 12100, \t Total Gen Loss : 25.826562881469727, \t Total Dis Loss : 0.0008407812565565109\n",
      "Steps : 12200, \t Total Gen Loss : 25.136323928833008, \t Total Dis Loss : 0.0013880161568522453\n",
      "Steps : 12300, \t Total Gen Loss : 22.77341079711914, \t Total Dis Loss : 0.0006186340469866991\n",
      "Steps : 12400, \t Total Gen Loss : 26.214515686035156, \t Total Dis Loss : 0.001636567641980946\n",
      "Steps : 12500, \t Total Gen Loss : 23.84168243408203, \t Total Dis Loss : 0.0007977460627444088\n",
      "Steps : 12600, \t Total Gen Loss : 26.932315826416016, \t Total Dis Loss : 0.0002686237567104399\n",
      "Steps : 12700, \t Total Gen Loss : 26.16956329345703, \t Total Dis Loss : 0.0007617500377818942\n",
      "Steps : 12800, \t Total Gen Loss : 21.548635482788086, \t Total Dis Loss : 0.0009908517822623253\n",
      "Steps : 12900, \t Total Gen Loss : 26.892070770263672, \t Total Dis Loss : 0.0009128790115937591\n",
      "Steps : 13000, \t Total Gen Loss : 23.70498275756836, \t Total Dis Loss : 0.0006111605907790363\n",
      "Steps : 13100, \t Total Gen Loss : 21.7899169921875, \t Total Dis Loss : 0.02210024558007717\n",
      "Steps : 13200, \t Total Gen Loss : 27.901512145996094, \t Total Dis Loss : 0.001307048718445003\n",
      "Steps : 13300, \t Total Gen Loss : 27.02895736694336, \t Total Dis Loss : 0.0031034769490361214\n",
      "Steps : 13400, \t Total Gen Loss : 27.18951416015625, \t Total Dis Loss : 0.0021510464139282703\n",
      "Steps : 13500, \t Total Gen Loss : 27.05261993408203, \t Total Dis Loss : 0.001375551801174879\n",
      "Steps : 13600, \t Total Gen Loss : 24.09691047668457, \t Total Dis Loss : 0.0011855809716507792\n",
      "Steps : 13700, \t Total Gen Loss : 23.173913955688477, \t Total Dis Loss : 0.0010150368325412273\n",
      "Steps : 13800, \t Total Gen Loss : 23.917564392089844, \t Total Dis Loss : 0.001783074694685638\n",
      "Steps : 13900, \t Total Gen Loss : 23.95807647705078, \t Total Dis Loss : 0.002558143110945821\n",
      "Steps : 14000, \t Total Gen Loss : 22.753141403198242, \t Total Dis Loss : 0.005217518657445908\n",
      "Steps : 14100, \t Total Gen Loss : 23.04425811767578, \t Total Dis Loss : 0.001738790888339281\n",
      "Steps : 14200, \t Total Gen Loss : 22.179290771484375, \t Total Dis Loss : 0.009194262325763702\n",
      "Steps : 14300, \t Total Gen Loss : 27.217315673828125, \t Total Dis Loss : 0.001097754924558103\n",
      "Steps : 14400, \t Total Gen Loss : 26.84137725830078, \t Total Dis Loss : 0.006078805308789015\n",
      "Steps : 14500, \t Total Gen Loss : 24.784889221191406, \t Total Dis Loss : 0.006632352713495493\n",
      "Steps : 14600, \t Total Gen Loss : 26.146522521972656, \t Total Dis Loss : 0.0015132708940654993\n",
      "Steps : 14700, \t Total Gen Loss : 24.060901641845703, \t Total Dis Loss : 0.00784722063690424\n",
      "Steps : 14800, \t Total Gen Loss : 22.872041702270508, \t Total Dis Loss : 0.004314601421356201\n",
      "Steps : 14900, \t Total Gen Loss : 24.479476928710938, \t Total Dis Loss : 0.052642401307821274\n",
      "Steps : 15000, \t Total Gen Loss : 24.004898071289062, \t Total Dis Loss : 0.009845647029578686\n",
      "Steps : 15100, \t Total Gen Loss : 25.084409713745117, \t Total Dis Loss : 0.006033428944647312\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
